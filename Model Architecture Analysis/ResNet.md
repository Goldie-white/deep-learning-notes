# ResNet

残差网络（Residual Network，简称 ResNet）是由 Kaiming He 等人在 2015 年提出的深度神经网络架构. 它通过引入“跳跃连接”（Skip Connection）或“快捷连接”（Shortcut Connection），允许网络学习残差映射，从而让网络**在变深的同时保持可训练性**. 

> Deep Residual Learning for Image Recognition, https://arxiv.org/pdf/1512.03385

### 动机：深度的诅咒——退化问题 (The Degradation Problem)

在 ResNet 提出之前，理论上认为越深的网络应该具有越强的表达能力. 然而实验发现，随着网络层数增加（例如从 20 层增加到 56 层），训练误差反而上升了. 

假设我们有一个浅层网络 A（比如 20 层），它已经达到了很好的性能. 现在我们构建一个更深的网络 B（比如 56 层），把 A 的网络架构复制过来，然后再在后面加上 36 个层. 

 * 理论上：网络 B 的性能至少应该等于网络 A（因为它可以完全包含 A 的解）. 
 * 实际上：普通的深层网络 B 训练出来的误差远高于 A. 

这不是过拟合（Overfitting），因为过拟合通常表现为训练误差低、测试误差高；而在退化问题中，深层网络的训练误差也比浅层网络高. 这说明深层网络遇到了严重的**优化困难 (Optimization Difficulty)**，它甚至无法学会复现浅层网络的表现. 

### 残差块 (The Residual Block)：The "Do No Harm" Principle

ResNet 的解决方案极其简洁，它改变了网络学习的目标：不再让网络直接学习目标映射 $H(x)$，而是改为学习残差 $F(x)$. 

假设我们希望网络层学习的目标映射为 $H(x)$. 

- 普通网络：直接尝试拟合 $H(x)$. 这相当于**每一层都试图重新构造一套特征表示**. 在浅层网络中，这尚可接受；但在上百层的网络中，要求每一层都“从头来过”，优化极其困难且不稳定. 

- ResNet：引入跳跃连接，将输入 $x$ 直接加到输出上. 网络实际上只需要学习残差函数 $F(x) := H(x) - x$. 因此，原目标映射变为：
  $$
  H(x) = F(x) + x
  $$
  其中 $x$ 是输入，$F(x)$ 是网络层学习到的非线性变换. 这相当于**在前面层提取出来的特征的基础上进行微调**. 

ResNet 的基本思想是：通过预设 $H(x) = F(x) + x$，我们将初始状态（当权重为 0 时）设定为恒等映射. 这为深层网络提供了一个“保底”性能——只有当非线性变换 $F(x)$ 确实能降低 Loss 时，网络才会去学习它；否则，它至少可以退化回恒等映射，保持浅层网络的性能. 这意味着，增加深度不会让模型变差（The "Do No Harm" Principle）. 

#### 为什么 ResNet 学习比传统网络容易？本质上是网络学习恒等映射远比零映射困难

到这里，也许读者还是会有疑问，为什么说不使用残差连接，每一层就要“从头来过“地去构造一套特征表示呢？直接去复制上一层的结果，然后稍作修改，这很难吗？

是的，对普通网络来说，这很难，以至于它们不擅长继承前面层的结果. **“直接去复制上一层的结果，然后稍作修改”**，这恰是 ResNet 才容易做到的事情，**是 ResNet 的 structural bias**. 

本质上，这是因为**网络学习恒等映射远比零映射困难**. 这个道理不难想清楚：

- 如果最优映射接近恒等映射，在普通网络中，由于非线性激活函数的存在，需要精确地将权重逼近某种特定配置以模拟恒等映射；
- 而在 ResNet 中，只需将权重推向 0（即让 $F(x) \to 0$），即可轻松实现 $H(x) \to x$. **学习零映射的难度远低于恒等映射**. 

正因为 ResNet 具有这种**容易继承浅层结果的能力**，相当于把恒等映射这个“保底解”直接写入了网络，使模型不至于因为层数加深而性能退化. 

一句话总结：换言之，在普通深层网络中，**继承并微调已有特征在优化上是高度不稳定的**，而 ResNet 通过显式的恒等通路，使这种继承变得结构性可行. 

### 为什么残差学习有效？

#### A. “保底”机制

这个原因，其实前面已经有很详细的论述了. 由于神经网络学习零映射的难度远低于恒等映射，因此对于 ResNet 来说，加深网络以后至少保持原有的性能是很容易的（“保底”机制），但对普通网络来说却很难. 

在极深的网络中，我们不应该把每一层看作是全新的特征提取器，而应看作是对特征的渐进式微调（Refinement）. 这就像雕刻：

- 普通网络：试图每一刀都直接砍出最终形状. 
- ResNet：先有一个大致轮廓（$x$），然后每一刀只是对之前的成果进行打磨（$F(x)$）. 

#### B. 改善梯度流

从反向传播的角度看，残差结构极大地改善了梯度流. 

在 ResNet 中，第 $l$ 个残差块的输出 $x_{l+1}$ 和输入 $x_l$ 的关系是：
$$
x_{l+1} = x_l + F(x_l, W_l)
$$
递推可得：
$$
x_L=x_l+\sum_{i=l}^{L-1}F(x_i,W_i),\ \forall L>l
$$
假设损失函数为 $\mathcal{L}$，根据链式法则，关于输入 $x_l$ 的梯度可以表示为：
$$
\frac{\partial \mathcal{L}}{\partial x_l} = \frac{\partial \mathcal{L}}{\partial x_L} \left( I + \frac{\partial}{\partial x_l} \sum_{i=l}^{L-1} F(x_i, \mathcal{W}_i) \right)
$$

- 公式中的 $I$ 保证了**深层的梯度信号可以畅通无阻地通过跳跃连接传回浅层**. 
- 这种结构**打破了传统网络中梯度的连乘衰减效应**：即便 $F(x)$ 部分的梯度很小，只要 $I$ 这一项存在，梯度就能有效回流. 这使得训练上百层甚至上千层的网络成为可能. 

#### C. Smoothing the Optimization Landscape

研究表明（如 *Visualizing the Loss Landscape of Neural Nets*, NIPS 2018），ResNet 的跳跃连接极大地平滑了损失函数的几何形状（Loss Landscape）. 

> *Visualizing the Loss Landscape of Neural Nets*, NIPS 2018, https://arxiv.org/pdf/1712.09913

- 普通深层网络：损失曲面非常崎岖，充满了非凸的局部极小值和鞍点. 如果网络试图学习恒等映射但不仅没学好，反而陷入了混乱的非线性变换中，梯度就会在这些崎岖的 landscape 中消失或爆炸. 
- 残差网络：由于 $x$ 可以直接流过，整个函数在初始化附近表现得更像一个线性系统（Linear-like behavior）. 这使得损失曲面变得更加平滑、凸性更好. 

### Takeaways

1. ResNet 改变了特征提取的范式，将学习目标从“全量重构特征”转变为对浅层特征的“渐进式微调”. 
2. ResNet 学习比传统网络容易，本质上是因为拟合零映射（$F(x) \to 0$）远比拟合恒等映射（$H(x) \to x$）容易. ResNet 通过引入跳跃连接，将恒等映射设为初始解，确立了“性能不下降”的保底机制（The "Do No Harm" Principle）. 
3. 残差结构改善了梯度流，有效防止了梯度消失，并平滑了损失函数的 Loss Landscape，使深层网络更易收敛. 